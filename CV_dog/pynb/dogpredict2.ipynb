{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['labels.csv', '.DS_Store', 'test', 'train', 'sample_submission.csv']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from os import listdir\n",
    "from os.path import basename,join,exists\n",
    "import os\n",
    "print(listdir(\"../input\"))\n",
    "import threading\n",
    "from queue import Queue\n",
    "from math import floor\n",
    "import time\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n",
      "['affenpinscher', 'afghan_hound', 'african_hunting_dog', 'airedale', 'american_staffordshire_terrier', 'appenzeller', 'australian_terrier', 'basenji', 'basset', 'beagle', 'bedlington_terrier', 'bernese_mountain_dog', 'black-and-tan_coonhound', 'blenheim_spaniel', 'bloodhound', 'bluetick', 'border_collie', 'border_terrier', 'borzoi', 'boston_bull', 'bouvier_des_flandres', 'boxer', 'brabancon_griffon', 'briard', 'brittany_spaniel', 'bull_mastiff', 'cairn', 'cardigan', 'chesapeake_bay_retriever', 'chihuahua', 'chow', 'clumber', 'cocker_spaniel', 'collie', 'curly-coated_retriever', 'dandie_dinmont', 'dhole', 'dingo', 'doberman', 'english_foxhound', 'english_setter', 'english_springer', 'entlebucher', 'eskimo_dog', 'flat-coated_retriever', 'french_bulldog', 'german_shepherd', 'german_short-haired_pointer', 'giant_schnauzer', 'golden_retriever', 'gordon_setter', 'great_dane', 'great_pyrenees', 'greater_swiss_mountain_dog', 'groenendael', 'ibizan_hound', 'irish_setter', 'irish_terrier', 'irish_water_spaniel', 'irish_wolfhound', 'italian_greyhound', 'japanese_spaniel', 'keeshond', 'kelpie', 'kerry_blue_terrier', 'komondor', 'kuvasz', 'labrador_retriever', 'lakeland_terrier', 'leonberg', 'lhasa', 'malamute', 'malinois', 'maltese_dog', 'mexican_hairless', 'miniature_pinscher', 'miniature_poodle', 'miniature_schnauzer', 'newfoundland', 'norfolk_terrier', 'norwegian_elkhound', 'norwich_terrier', 'old_english_sheepdog', 'otterhound', 'papillon', 'pekinese', 'pembroke', 'pomeranian', 'pug', 'redbone', 'rhodesian_ridgeback', 'rottweiler', 'saint_bernard', 'saluki', 'samoyed', 'schipperke', 'scotch_terrier', 'scottish_deerhound', 'sealyham_terrier', 'shetland_sheepdog', 'shih-tzu', 'siberian_husky', 'silky_terrier', 'soft-coated_wheaten_terrier', 'staffordshire_bullterrier', 'standard_poodle', 'standard_schnauzer', 'sussex_spaniel', 'tibetan_mastiff', 'tibetan_terrier', 'toy_poodle', 'toy_terrier', 'vizsla', 'walker_hound', 'weimaraner', 'welsh_springer_spaniel', 'west_highland_white_terrier', 'whippet', 'wire-haired_fox_terrier', 'yorkshire_terrier']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../input/sample_submission.csv')\n",
    "train_dir_path = \"../input/train\"\n",
    "test_dir_path = \"../input/test\"\n",
    "#pickled_dir_path  = \"../output/pickled_Data\"\n",
    "labels_df = pd.read_csv('../input/labels.csv')\n",
    "dog_breeds = list(df.columns[1:])\n",
    "print(len(dog_breeds))\n",
    "print(dog_breeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10222\n",
      "10357\n"
     ]
    }
   ],
   "source": [
    "train_img_fpaths = [ join(train_dir_path, f) for f in listdir(train_dir_path)]\n",
    "test_img_fpaths = [join(test_dir_path, f) for f in listdir(test_dir_path)]\n",
    "print(len(train_img_fpaths))\n",
    "print(len(test_img_fpaths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dog_breed_from_id(dog_id):\n",
    "    #labels_df = pd.read_csv('../input/labels.csv')\n",
    "    return labels_df[labels_df['id'] ==dog_id]['breed'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables \n",
    "IMG_HEIGHT = 150\n",
    "IMG_WIDTH = 150\n",
    "IMG_CHANNELS = 3\n",
    "BATCH_SIZE = 500\n",
    "lock = threading.Lock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_to_array(img_path):   \n",
    "    img_array = cv.imread(img_path)\n",
    "    img_array = cv.resize(img_array, (IMG_HEIGHT, IMG_WIDTH))\n",
    "    img_array = img_array.reshape(-1,IMG_HEIGHT,IMG_WIDTH, IMG_CHANNELS)\n",
    "    return img_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize queue which is threadsafe \n",
    "def initialize_queue():\n",
    "    queue =Queue()\n",
    "    return queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of image ids from names of test images\n",
    "def get_test_image_ids():\n",
    "    return [basename(fpath).split('.')[0] for fpath in test_img_fpaths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts image files to numpy array and based on train/test, return train array and labels,\n",
    "def get_data(is_train):\n",
    "    # 1 batch per thread and last thread with remaining images\n",
    "    img_fpaths = train_img_fpaths if is_train else test_img_fpaths\n",
    "    num_threads = floor(len(img_fpaths)/BATCH_SIZE)\n",
    "    print(\"num of threads:\", num_threads + 1)\n",
    "    img_array = None\n",
    "    queue = initialize_queue()\n",
    "    results = []          # results from multiple threads\n",
    "    print(\"getting training data....\") if is_train else print(\"getting testing data....\")\n",
    "    \n",
    "    # load queue with data for each task\n",
    "    for batch_index in range(num_threads + 1):\n",
    "        if batch_index == num_threads:\n",
    "            file_batch = img_fpaths[(batch_index*BATCH_SIZE):]\n",
    "        else:\n",
    "            file_batch = img_fpaths[(batch_index*BATCH_SIZE) : (batch_index + 1)*BATCH_SIZE]\n",
    "        queue.put(file_batch)\n",
    "    \n",
    "    # iterate over loop to create threads\n",
    "    for thread_index in range(num_threads+1):\n",
    "        thread = threading.Thread(target = get_train_data_parallely, args=(queue, results)) if is_train else threading.Thread(target =get_testing_data_parallely, args =(queue, results))    \n",
    "        thread.start()\n",
    "        print(\"{} started\".format(thread.name))\n",
    "       # worker_threads.append(thread)\n",
    "        \n",
    "    # when queue in empty\n",
    "    queue.join()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert training data into list of tuples\n",
    "# this subroutine represents a task for child thread to collect training data\n",
    "def get_train_data_parallely(queue, results):\n",
    "    result = []\n",
    "    while not queue.empty():\n",
    "        fpaths = queue.get()\n",
    "        for f_path in fpaths:\n",
    "            img_array = img_to_array(f_path)\n",
    "            # train_img_array = img_array if train_img_array is None else np.vstack((train_img_array, img_array))\n",
    "            img_name = basename(f_path)\n",
    "            img_id = img_name.split('.')[0]\n",
    "            dog_breed = dog_breed_from_id(img_id)\n",
    "            #train_labels.append(dog_breed)\n",
    "            results.append((img_array, dog_breed))\n",
    "            \n",
    "    # append arr,labels for current task to results\n",
    "    print(\"{} finished\".format(threading.currentThread().getName()))\n",
    "    # signal for task has been done\n",
    "    queue.task_done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# worker job for converting test imgs to array\n",
    "def get_testing_data_parallely(queue, results):\n",
    "    while not queue.empty():\n",
    "        file_batch = queue.get()\n",
    "        for f_path in file_batch:        \n",
    "            img_name = basename(f_path)\n",
    "            img_id = img_name.split('.')[0]\n",
    "            results.append((img_id, img_to_array(f_path)))\n",
    "    print(\"{} finished\".format(threading.currentThread().getName()))\n",
    "    queue.task_done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method for getting training data\n",
    "def get_training_data():\n",
    "    train_results = get_data(is_train = True)\n",
    "    train_labels = []\n",
    "    img_arrays= []\n",
    "    for u_index in range(len(train_results)):\n",
    "        img_arr, identified_breed = train_results[u_index]\n",
    "        img_arrays.append(img_arr)\n",
    "        train_labels.append(identified_breed)\n",
    "    train_arr = np.array(img_arrays).reshape(-1, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)\n",
    "    train_arr = train_arr/255\n",
    "    train_labels = one_hot_encode_labels(train_labels)\n",
    "    return train_arr,train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method for getting testing arr\n",
    "def get_testing_data():\n",
    "    results = get_data(is_train = False)\n",
    "    test_img_ids = []\n",
    "    test_img_list = []\n",
    "    for test_result in results:\n",
    "        img_id, img_arr = test_result\n",
    "        test_img_list.append(img_arr)\n",
    "        test_img_ids.append(img_id)\n",
    "    test_img_arr = np.array(test_img_list).reshape(-1, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)\n",
    "    test_img_arr = test_img_arr/255\n",
    "    return test_img_arr, test_img_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj_to_disk(fname, obj):\n",
    "    print(\"saving \"+ fname +\" to filesystem\")\n",
    "    if  exists(fname):\n",
    "        print(fname + \"already exists\") \n",
    "    with open(fname, 'wb') as f:\n",
    "        pickle.dump(obj, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_obj_from_disk(fname):\n",
    "    if exists(fname):\n",
    "        print(\"loading \"+fname + \" from filesystem\")\n",
    "        obj = None\n",
    "        with open(fname, 'rb') as f:\n",
    "            obj = pickle.load(f)\n",
    "        return obj\n",
    "    else:\n",
    "        print(fname + \"doesnt not exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_test_data(load_train=False, load_test=False, one_hot_encode=False):\n",
    "    train_arr = None\n",
    "    train_labels = None\n",
    "    test_arr = None\n",
    "    \n",
    "    # check if training data and labels exists already as pickled file\n",
    "    if load_train:\n",
    "        if exists(\"train_data.pickle\") and exists(\"train_labels.pickle\"):\n",
    "            train_arr = load_obj_from_disk(\"train_data.pickle\")\n",
    "            train_labels = load_obj_from_disk(\"train_labels.pickle\")\n",
    "            if one_hot_encode:\n",
    "                train_labels = one_hot_encode_labels(train_labels)\n",
    "        else:\n",
    "            # create training_data and save it to filesystem\n",
    "            train_arr, train_labels = get_data(is_train= True)\n",
    "            if not exists(\"train_data.pickle\"):\n",
    "                save_obj_to_disk(\"train_data.pickle\", train_data)\n",
    "            if not exists(\"train_labels.pickle\"):\n",
    "                save_obj_to_disk(\"train_labels.pickle\", train_labels)\n",
    "        print(\" train array shape : {}, train array labels: {}\".format(train_data.shape,len(train_labels)))\n",
    "        \n",
    "    # check if testing data and labels exists already as pickled file\n",
    "    if load_test:\n",
    "        if exists(\"test_data.pickle\"):\n",
    "            test_arr = load_obj_from_disk(\"test_data.pickle\")\n",
    "        else:\n",
    "            # create test_data and save it to filesystem\n",
    "            test_arr = get_data(is_train= False)\n",
    "            save_obj_to_disk(\"test_data.pickle\", test_arr)\n",
    "        print(\" test array shape : {}\".format(test_arr.shape))\n",
    "    return train_arr, train_labels, test_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method for onehot encoding labels of train_arr\n",
    "def one_hot_encode_labels(label_arr):\n",
    "    from sklearn.preprocessing import LabelEncoder ,OneHotEncoder\n",
    "    labelEncoder = LabelEncoder()\n",
    "    integer_encoded = labelEncoder.fit_transform(np.array(label_arr))\n",
    "    integer_encoded = integer_encoded.reshape(-1,1)\n",
    "    onehotEncoder = OneHotEncoder()\n",
    "    onehot_encoded_arr = onehotEncoder.fit_transform(integer_encoded).toarray()\n",
    "    return onehot_encoded_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of threads: 21\n",
      "getting training data....\n",
      "Thread-4 started\n",
      "Thread-5 started\n",
      "Thread-6 started\n",
      "Thread-7 started\n",
      "Thread-8 started\n",
      "Thread-9 started\n",
      "Thread-10 started\n",
      "Thread-11 started\n",
      "Thread-12 started\n",
      "Thread-13 started\n",
      "Thread-14 started\n",
      "Thread-15 started\n",
      "Thread-16 started\n",
      "Thread-17 started\n",
      "Thread-18 started\n",
      "Thread-19 started\n",
      "Thread-20 started\n",
      "Thread-21 started\n",
      "Thread-22 started\n",
      "Thread-23 started\n",
      "Thread-24 started\n",
      "Thread-24 finished\n",
      "Thread-12 finished\n",
      "Thread-13 finished\n",
      "Thread-8 finished\n",
      "Thread-19 finished\n",
      "Thread-21 finished\n",
      "Thread-9 finished\n",
      "Thread-4 finished\n",
      "Thread-23 finished\n",
      "Thread-5 finished\n",
      "Thread-20 finished\n",
      "Thread-15 finished\n",
      "Thread-18 finished\n",
      "Thread-16 finished\n",
      "Thread-17 finished\n",
      "Thread-7 finished\n",
      "Thread-22 finished\n",
      "Thread-6 finished\n",
      "Thread-10 finished\n",
      "Thread-11 finished\n",
      "Thread-14 finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y =get_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of threads: 21\n",
      "getting testing data....\n",
      "Thread-25 started\n",
      "Thread-26 started\n",
      "Thread-27 started\n",
      "Thread-28 started\n",
      "Thread-29 started\n",
      "Thread-30 started\n",
      "Thread-31 started\n",
      "Thread-32 started\n",
      "Thread-33 started\n",
      "Thread-34 started\n",
      "Thread-35 started\n",
      "Thread-36 started\n",
      "Thread-37 started\n",
      "Thread-38 started\n",
      "Thread-39 started\n",
      "Thread-40 started\n",
      "Thread-41 started\n",
      "Thread-42 started\n",
      "Thread-43 started\n",
      "Thread-44 started\n",
      "Thread-45 started\n",
      "Thread-45 finished\n",
      "Thread-44 finished\n",
      "Thread-26 finished\n",
      "Thread-39 finished\n",
      "Thread-31 finished\n",
      "Thread-41 finished\n",
      "Thread-33 finished\n",
      "Thread-25 finished\n",
      "Thread-40 finished\n",
      "Thread-32 finished\n",
      "Thread-36 finished\n",
      "Thread-43 finished\n",
      "Thread-35 finished\n",
      "Thread-37 finished\n",
      "Thread-38 finished\n",
      "Thread-28 finished\n",
      "Thread-27 finished\n",
      "Thread-29 finished\n",
      "Thread-34 finished\n",
      "Thread-42 finished\n",
      "Thread-30 finished\n"
     ]
    }
   ],
   "source": [
    "test_x, test_img_ids = get_testing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10222, 150, 150, 3)\n",
      "(10222, 120)\n",
      "(10357, 150, 150, 3)\n",
      "10357\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(len(test_img_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import required packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN model\n",
    "model = Sequential()\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "# conv 1\n",
    "model.add(Conv2D(16, (3,3), input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)))       # input -N,150,150,3, output- N,148,148,16\n",
    "model.add(BatchNormalization(axis=3))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "\n",
    "# max pool 1\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=2))                                   #input- N,148,148,16, output- N, 74,74,16\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "# # conv 2\n",
    "model.add(Conv2D(32, (3,3)))                                                         #input- N,74,74,16 output - N, 72,72,16\n",
    "model.add(BatchNormalization(axis=3))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "\n",
    "# max pool 2\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=2))                                 #input - N,72,72,16, output- N,36,36,16\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "# conv 3\n",
    "model.add(Conv2D(48, (3,3)))                                                       #input - N,36,36,16, output- N,34,34,32\n",
    "model.add(BatchNormalization(axis=3))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.7))\n",
    "\n",
    "# max pool 3\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=2))                                #input- N,34,34,32, output- N,17,17,32\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "# # conv 4\n",
    "model.add(Conv2D(64, (3,3)))                                                     #input- N,17,17,32, output- N,15,15,32\n",
    "model.add(BatchNormalization(axis=3))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.7))\n",
    "# max pool 4\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=2))                              #input- N,15,15,32, output- N,7,7,32\n",
    "\n",
    "# flatten\n",
    "model.add(Flatten())                                                            # output- 1568\n",
    "\n",
    "# fc layer 1\n",
    "model.add(Dense(1024, activation='relu'))                                  \n",
    "\n",
    "# fc layer 2\n",
    "model.add(Dense(512, activation='relu'))\n",
    "\n",
    "# fc layer 3\n",
    "model.add(Dense(256, activation='relu'))\n",
    "\n",
    "# fc layer 4\n",
    "model.add(Dense(120, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 148, 148, 16)      448       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 148, 148, 16)      64        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 148, 148, 16)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 74, 74, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 72, 72, 32)        4640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 72, 72, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 72, 72, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 36, 36, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 34, 34, 48)        13872     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 34, 34, 48)        192       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 34, 34, 48)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 17, 17, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 15, 15, 64)        27712     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 15, 15, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              3212288   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 120)               30840     \n",
      "=================================================================\n",
      "Total params: 3,946,568\n",
      "Trainable params: 3,946,248\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model for with softmax cross entropy and adam optimizer, set accuracy as parameter to evaluate\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " 2656/10222 [======>.......................] - ETA: 9:22 - loss: 4.9055 - acc: 0.0087"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-26-8ba3e28714b6>\", line 2, in <module>\n",
      "    model_hist = model.fit(train_x, train_y, batch_size=32, nb_epoch=20, verbose=1)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\", line 1037, in fit\n",
      "    validation_steps=validation_steps)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\", line 199, in fit_loop\n",
      "    outs = f(ins_batch)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 2672, in __call__\n",
      "    return self._legacy_call(inputs)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 2654, in _legacy_call\n",
      "    **self.session_kwargs)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 895, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1128, in _run\n",
      "    feed_dict_tensor, options, run_metadata)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1344, in _do_run\n",
      "    options, run_metadata)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1350, in _do_call\n",
      "    return fn(*args)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1329, in _run_fn\n",
      "    status, run_metadata)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1828, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1090, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/anaconda3/lib/python3.6/inspect.py\", line 1483, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/anaconda3/lib/python3.6/inspect.py\", line 1441, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/anaconda3/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/anaconda3/lib/python3.6/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/anaconda3/lib/python3.6/posixpath.py\", line 386, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/anaconda3/lib/python3.6/posixpath.py\", line 420, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/anaconda3/lib/python3.6/posixpath.py\", line 169, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "# train model on training data\n",
    "model_hist = model.fit(train_x, train_y, batch_size=32, nb_epoch=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_x, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions.shape)\n",
    "print(len(dog_breeds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "submission_res = pd.DataFrame(data= predictions, index =test_img_ids, columns= dog_breeds)\n",
    "submission_res.index.name = 'id'\n",
    "submission_res.to_csv('submission.csv', encoding='utf-8', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(model_hist.history['acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(model_hist.history['loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
